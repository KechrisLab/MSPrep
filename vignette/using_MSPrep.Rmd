---
title: "Using MSPrep"
author: 
    - name: Matt Mulvahill
      email: matthew.mulvahill@ucdenver.edu
      affiliation: UC Anschutz
    - name: Grant Hughes
      email: fill@me.in
      affiliation: National Jewish Hospital
    - name: Sean Jacobson
      email: jacobsons@njhealth.org
      affiliation: National Jewish Hospital
    - name: Katerina Kechris
      email: katerina.kechris@ucdenver.edu
      affiliation: UC Anschutz
    - name: Harrison Pielke-Lombardo
      email: harrison.pielke-lombardo@ucdenver.edu
      affiliation: UC Anschutz
package: MSPrep
output: 
  BiocStyle::html_document:
    highlight: "tango"
    code_folding: show
    toc: true
    toc_float: 
      collapsed: false
vignette: |
  %\VignetteIndexEntry{Using MSPrep}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
---

```{r, include=FALSE, echo=FALSE}
# date: "`r doc_date()`"
# "`r pkg_ver('BiocStyle')`"
# <style>
#     pre {
#     white-space: pre !important;
#     overflow-y: scroll !important;
#     height: 50vh !important;
#     }
# </style>
```

# Introduction

MSPrep provides the five key functions for preparing metabolomics data for analysis. This vignette will provide
1. an explanation what each of the functions does
2. suggestions for how to select parameters
3. an explanation of how to use the final outputted data
4. code to perform all of the steps in a pipeline

# Loading libraries

```{r}
library(MSPrep)
library(tidyverse)
```

# Getting our columns in order

In order to allow MSPrep to do the heavy lifting, we first need to get our data into the correct format. Most mass spectrometry data will have a similar format, so not much should be required to get it ready.

Two columns are for the mass-to-charge ratio and the retention-time. They can have any name you'd like (usually "mz" and "rt").

The other columns are for the other variables. These column names contain information about their contents which will be used later in the pipeline. There are three pieces of information which need to be present at the end of each of the column name, separated by a consistent separator. These are the spike, batch, and replicate ID.

As an example take a look at the provided dataset, msquant.

```{r}
data("msquant")
strsplit(colnames(msquant)[3], "_")
```

The third column name is `colnames(msquant)[3]`. The first part "Neutral_Operator_Dif_Pos_" won't be used, so we will assign it to col_extra_txt. The next value, "1x", is the spike. The following value, "O1", is the batch. The remaining values, "A" and "01", are the replicate and subject IDs.

Identifiers in the datasets
- LCMS_Run_ID = operator/replicate (A-C), subject (01-03), concentration  (1x,2x,4x)
- SubjectID   = subject (01-03), concentration  (1x,2x,4x)

With our data in this format, we can start the pipeline.

# Tidying the data

The first step is to extract the information from the column names discussed above, and get it into a tidy data frame. We can do this with the ms_tidy function.

```{r}
tidied_data = ms_tidy(msquant, mz = "mz", rt = "rt", col_extra_txt = "Neutral_Operator_Dif_Pos_", separator = "_", col_names = c("spike", "batch", "replicate", "subject_id"))
```

Note, the names chosen for col_name are arbitratry, but we will use them later on.

# Preparing the data

This step summarizes the technical replicates. It does this using the following procedure for each compound in each batch.

1. If there are less than a minimum proportion of the values found among the replicates (usually one or zero), leave value empty. Otherwise proceed.
1. Calculate the coefficient of variation between the replicates using $c_v = \frac{\sigma}{\mu}$.
2. For three replicates, if the coefficient of variation is above a specified level, use the median value for the compound, to correct for the large dispersion.
3. Otherwise, use the mean value of the replicates for the compound.

```{r}
summarized_data = ms_prepare(tidied_data, mz = "mz", rt = "rt", replicate = "replicate", batch = "batch", groupingvars = "spike", subject_id = "subject_id", cvmax = 0.50, min_proportion_present = 1/3, missing_val = 1)
```

# Filtering missing compounds

This step is pretty straight forward but very important. Simply supply a percentage of the number of samples for which a compound needs to have data present for in order to be kept.

```{r}
filtered_data = ms_filter(summarized_data, filter_percent=0.8)
```

# Imputing missing values

Next, you'll need to make a decision about how you'd like to handle the remaining missing data.There are three methods provided. 
1. half-min (half the minimum value)
2. bpca (Bayesian PCA), 
3. knn (k-nearest neighbors)

Half-min is the fastest and is often sufficient. KNN typically takes the longest. If you choose to use KNN, you can provide a value for k.

```{r}
imputed_data = ms_impute(filtered_data, method ="knn", k = 5)
```

# Normalizing the data

In order to make comparisons between compounds, the data need to be normalized. This step performs one of six normalization strategies. 
1. ComBat ()
1. quantile + ComBat
2. median + ComBat
3. CRMN ()
4. RUV ()
5. SVA ()

For experiments which have control compounds, a list of the column numbers containing them should be provided in the controls parameter. Otherwise, simply leave the controls parameter blank or NULL. 

```{r}
normalized_data = ms_normalize(imputed_data, method ="CRMN", controls = NULL,  n_comp = 2, n_control = 10)
```

# Final output

# Performing all steps as a pipline

Often, you will want to just perform the whole pipeline. This can easily be done in a single statement using the %>% operator from the magrittr package. This threads the result of each function into the first position of the next.

```{r}
dat <-
  msquant %>%
  ms_tidy %>%
  ms_prepare(replicate = "replicate", batch = "batch", groupingvars = "spike") %>%
  ms_filter(0.8) %>%
  ms_impute(method = "halfmin") %>%
  ms_normalize(method = "quantile + ComBat")
```